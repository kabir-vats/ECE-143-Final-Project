{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "467a4cbb-61b6-4439-b525-5216e39082ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/kvats/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/kvats/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/kvats/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import os\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8cb2afba-2cfe-4c20-a2ec-85b1c364c7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_n_words = 200\n",
    "def trim_string(x):\n",
    "\n",
    "    x = x.split(maxsplit=first_n_words)\n",
    "    x = ' '.join(x[:first_n_words])\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cccf9530-df2e-4a59-9c56-77f26a63643d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_split(path='raw/',ratio=(0.7,0.15,0.15))->pd.DataFrame:\n",
    "    \"\"\"split raw csv files into train, validation and test sets\n",
    "\n",
    "    Args:\n",
    "        path (str, optional): path of raw files. Defaults to '../raw/'.\n",
    "        ratio (tuple, optional): splitting ratio. Defaults to (0.7,0.15,0.15).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: train, validation and test sets\n",
    "    \"\"\"    \n",
    "    assert sum(ratio)==1.0 and len(ratio)==3, \"ratio error\"\n",
    "    true_df = pd.read_csv(path+'Fake.csv')\n",
    "    fake_df = pd.read_csv(path+'True.csv')\n",
    "    true_df[\"label\"] = 1\n",
    "    fake_df[\"label\"] = 0\n",
    "    df = pd.concat([true_df, fake_df], ignore_index=True)\n",
    "    df['titletext'] = df['title'] + \". \" + df['text']\n",
    "    df['text'] = df['text'].apply(trim_string)\n",
    "    df['titletext'] = df['titletext'].apply(trim_string) \n",
    "    df = df.reindex(columns=['label', 'title', 'text', 'titletext'])\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    train_df, temp_df = train_test_split(df, test_size=ratio[2]+ratio[1], random_state=42, stratify=df[\"label\"])\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=ratio[2]/(ratio[2]+ratio[1]), random_state=42, stratify=temp_df[\"label\"])\n",
    "    return train_df,val_df,test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bf350c1-833e-4641-a479-02cca15eff20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/kvats/private/nw/ECE-143-Final-Project'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('..')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a816667d-f364-4e30-b7f3-89988ceff6bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf1d690-1886-4d2e-a54e-bd68c0dc0929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39ef7d21-e978-47ae-81a1-314e11abe820",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val, df_test = dataset_split(ratio = (0.1, 0.8, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f736be90-125b-49c8-91f9-f48442c14feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing puctuiation\n",
    "def remove_punctuations(string):\n",
    "    return ''.join(c for c in string if c not in punctuation)\n",
    "\n",
    "#removing stopword\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stopwords(string):    \n",
    "    tokenized = word_tokenize(string)\n",
    "    filtered_sentence = [word for word in tokenized if not word in stop_words]\n",
    "    return ' '.join(c for c in filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ecc70c9-5525-4f7d-af7a-25039a4e18f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to lowercase\n",
    "df_train['titletext'] = df_train.apply(lambda row: str(row['titletext']).lower(), axis=1)\n",
    "df_test['titletext'] = df_test.apply(lambda row: str(row['titletext']).lower(), axis=1)\n",
    "\n",
    "# apply remove punctuation\n",
    "df_train['titletext'] = df_train.apply(lambda row: remove_punctuations(row['titletext']), axis=1)\n",
    "df_test['titletext'] = df_test.apply(lambda row: remove_punctuations(row['titletext']), axis=1)\n",
    "\n",
    "# apply remove stopword\n",
    "df_train['titletext'] = df_train.apply(lambda row: remove_stopwords(row['titletext']), axis=1)\n",
    "df_test['titletext'] = df_test.apply(lambda row: remove_stopwords(row['titletext']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "188c7348-b47a-491b-b2fa-d2272db3ee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(min_df=2, ngram_range=(1, 1))\n",
    "\n",
    "X_train = vect.fit(df_train['titletext']).transform(df_train['titletext']) \n",
    "X_test = vect.transform(df_test['titletext'])\n",
    "\n",
    "y_train = df_train['label']\n",
    "y_test = df_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d235b49-c395-460b-92f6-b8d8b6afec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [\n",
    "  {'penalty': ['l2'], 'solver': ['lbfgs'], 'C' : [0.5]},\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a938938-9112-44f8-928d-a634feea72d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "tuning_model = GridSearchCV(lr, parameters, cv=5, verbose=2)\n",
    "tuning_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615dbbd9-f84a-4316-a701-69bada8a030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best parameters: {}\".format(tuning_model.best_params_))\n",
    "print(\"best score:      {:0.5f}\".format(tuning_model.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3748a8-20cb-43d1-bcc6-adc2ca71a052",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
