{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e69ff2d4-047a-4fc0-804d-fe016bf172e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.44.2)\n",
      "Requirement already satisfied: sentencepiece in /home/kvats/.local/lib/python3.11/site-packages (0.2.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.25.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d835fb85-b950-4442-9e30-27aa17b55a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "550333db-9a9c-4873-bc95-36c4b6c97e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device is: cuda\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import copy\n",
    "import gc\n",
    "import itertools\n",
    "import joblib\n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import scipy as sp\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
    "\n",
    "# ======= OPTIONS =========\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Current device is: {device}\")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc7543c8-d2e2-4a5a-b262-d1a314f070d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    APEX = True # Automatic Precision Enabled\n",
    "    BATCH_SCHEDULER = True\n",
    "    BATCH_SIZE_TRAIN = 32\n",
    "    BATCH_SIZE_VALID = 16\n",
    "    BETAS = (0.9, 0.999)\n",
    "    DEBUG = False\n",
    "    DECODER_LR = 2e-5\n",
    "    ENCODER_LR = 2e-5\n",
    "    EPOCHS = 5\n",
    "    EPS = 1e-6\n",
    "    FOLDS = 4\n",
    "    GRADIENT_ACCUMULATION_STEPS = 1\n",
    "    GRADIENT_CHECKPOINTING = True\n",
    "    MAX_GRAD_NORM=1000\n",
    "    MAX_LEN = 512\n",
    "    MIN_LR = 1e-6\n",
    "    MODEL = \"microsoft/deberta-v3-base\"\n",
    "    NUM_CYCLES = 0.5\n",
    "    NUM_WARMUP_STEPS = 0\n",
    "    NUM_WORKERS = multiprocessing.cpu_count()\n",
    "    PRINT_FREQ = 20\n",
    "    SCHEDULER = 'cosine' # ['linear', 'cosine']\n",
    "    SEED = 27\n",
    "    TRAIN = True\n",
    "    TRAIN_FOLDS = [0, 1, 2, 3]\n",
    "    WANDB = False\n",
    "    WEIGHT_DECAY = 0.01\n",
    "\n",
    "class paths:\n",
    "    OUTPUT_PATH = \"./model/BERT\"\n",
    "\n",
    "if config.DEBUG:\n",
    "    config.EPOCHS = 2\n",
    "    config.TRAIN_FOLDS = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "699d5859-a882-4c5d-8b4a-e67dd204a6ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/kvats/private/nw/ECE-143-Final-Project'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('..')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0db3470b-9d87-491f-aa22-23441b354ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config_dict(config):\n",
    "    \"\"\"\n",
    "    Return the config, which is originally a class, as a Python dictionary.\n",
    "    \"\"\"\n",
    "    config_dict = dict((key, value) for key, value in config.__dict__.items() \n",
    "    if not callable(value) and not key.startswith('__'))\n",
    "    return config_dict\n",
    "\n",
    "\n",
    "def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "         'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "         'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "    ]\n",
    "    return optimizer_parameters\n",
    "\n",
    "\n",
    "def get_logger(filename):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "    if cfg.SCHEDULER == 'linear':\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=cfg.NUM_WARMUP_STEPS,\n",
    "            num_training_steps=num_train_steps\n",
    "        )\n",
    "    elif cfg.SCHEDULER == 'cosine':\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=cfg.NUM_WARMUP_STEPS,\n",
    "            num_training_steps=num_train_steps, num_cycles=cfg.NUM_CYCLES\n",
    "        )\n",
    "    return scheduler\n",
    "    \n",
    "\n",
    "def get_score(y_trues, y_preds):\n",
    "    score = roc_auc_score(y_trues, y_preds)\n",
    "    return score\n",
    "\n",
    "\n",
    "def seed_everything(seed=20):\n",
    "    \"\"\"Seed everything to ensure reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "\n",
    "def sep():\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))  \n",
    "    \n",
    "LOGGER = get_logger(\"./model/BERT\")\n",
    "seed_everything(seed=config.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c62ad15c-cb52-4439-b82d-7186fda51398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_split(path='raw/',ratio=(0.7,0.15,0.15))->pd.DataFrame:\n",
    "    \"\"\"split raw csv files into train, validation and test sets\n",
    "\n",
    "    Args:\n",
    "        path (str, optional): path of raw files. Defaults to '../raw/'.\n",
    "        ratio (tuple, optional): splitting ratio. Defaults to (0.7,0.15,0.15).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: train, validation and test sets\n",
    "    \"\"\"    \n",
    "    assert sum(ratio)==1.0 and len(ratio)==3, \"ratio error\"\n",
    "    true_df = pd.read_csv(path+'Fake.csv')\n",
    "    fake_df = pd.read_csv(path+'True.csv')\n",
    "    true_df[\"label\"] = 1\n",
    "    fake_df[\"label\"] = 0\n",
    "    df = pd.concat([true_df, fake_df], ignore_index=True)\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    train_df, temp_df = train_test_split(df, test_size=ratio[2]+ratio[1], random_state=42, stratify=df[\"label\"])\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=ratio[2]/(ratio[2]+ratio[1]), random_state=42, stratify=temp_df[\"label\"])\n",
    "    return train_df,val_df,test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67b14102-bfe7-480a-91ec-098e07689c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = train_df, val_df, test_df = dataset_split(ratio=(0.8, 0.1, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f08d5a5b-ee77-4063-8f94-8ea8eead002c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold  label\n",
      "0.0   1        2792\n",
      "      0        2587\n",
      "1.0   1        2791\n",
      "      0        2587\n",
      "2.0   1        2791\n",
      "      0        2587\n",
      "3.0   1        2791\n",
      "      0        2587\n",
      "4.0   1        2791\n",
      "      0        2587\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Clinton opposition to Asia trade pact 'close c...</td>\n",
       "      <td>Anyone with any sense of history realizes that...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>October 11, 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A MUST READ: OBAMA’S TREASON GOES INTO OVERDRIVE</td>\n",
       "      <td>Our culture is what makes this Nation great an...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Apr 6, 2015</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Probe of Clinton's email driven by facts, not ...</td>\n",
       "      <td>It s no secret that Senator John McCain hates ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>June 30, 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scott Baio Tries To Push Hillary’s ‘Body Doubl...</td>\n",
       "      <td>Yesterday, Democratic nominee Hillary Clinton ...</td>\n",
       "      <td>News</td>\n",
       "      <td>September 12, 2016</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alec Baldwin Just Put Trump’s Idiot Supporters...</td>\n",
       "      <td>Saturday Night Live returned with a stellar co...</td>\n",
       "      <td>News</td>\n",
       "      <td>April 9, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title                                               text       subject                date  label  fold\n",
       "0  Clinton opposition to Asia trade pact 'close c...  Anyone with any sense of history realizes that...  politicsNews   October 11, 2016       0   0.0\n",
       "1   A MUST READ: OBAMA’S TREASON GOES INTO OVERDRIVE  Our culture is what makes this Nation great an...      politics         Apr 6, 2015      1   0.0\n",
       "2  Probe of Clinton's email driven by facts, not ...  It s no secret that Senator John McCain hates ...  politicsNews      June 30, 2016       0   0.0\n",
       "3  Scott Baio Tries To Push Hillary’s ‘Body Doubl...  Yesterday, Democratic nominee Hillary Clinton ...          News  September 12, 2016      1   0.0\n",
       "4  Alec Baldwin Just Put Trump’s Idiot Supporters...  Saturday Night Live returned with a stellar co...          News       April 9, 2017      1   0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "X = train_df.loc[:, train_df.columns != \"label\"]\n",
    "y = train_df.loc[:, train_df.columns == \"label\"]\n",
    "\n",
    "for i, (train_index, valid_index) in enumerate(skf.split(X, y)):\n",
    "    train_df.loc[valid_index, \"fold\"] = i\n",
    "    \n",
    "print(train_df.groupby(\"fold\")[\"label\"].value_counts())\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acd2f496-a3a8-4253-9d3d-b70f4eb5a2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DebertaV2TokenizerFast(name_or_path='microsoft/deberta-v3-base', vocab_size=128000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t128000: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.MODEL)\n",
    "tokenizer.save_pretrained(paths.OUTPUT_DIR + '/tokenizer/')\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76782437-3316-4b7d-8db8-c5710e0d9676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a882744c944d4730ab3682799bf77aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26891 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_len: 512\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lengths = []\n",
    "tqdm_loader = tqdm(train_df['text'].fillna(\"\").values, total=len(train_df))\n",
    "for text in tqdm_loader:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "    lengths.append(length)\n",
    "    \n",
    "# config.MAX_LEN = max(lengths) + 3 # cls & sep & sep\n",
    "LOGGER.info(f\"max_len: {config.MAX_LEN}\")\n",
    "_ = plt.hist(lengths, bins=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4a0b616-9b9a-4288-9cb2-ff2fe48586cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(cfg, text, tokenizer):\n",
    "    \"\"\"\n",
    "    This function tokenizes the input text with the configured padding and truncation. Then,\n",
    "    returns the input dictionary, which contains the following keys: \"input_ids\",\n",
    "    \"token_type_ids\" and \"attention_mask\". Each value is a torch.tensor.\n",
    "    :param cfg: configuration class with a TOKENIZER attribute.\n",
    "    :param text: a numpy array where each value is a text as string.\n",
    "    :return inputs: python dictionary where values are torch tensors.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text, \n",
    "        return_tensors=None, \n",
    "        add_special_tokens=True, \n",
    "        max_length=cfg.MAX_LEN,\n",
    "        padding='max_length', # TODO: check padding to max sequence in batch\n",
    "        truncation=True\n",
    "    )\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long) # TODO: check dtypes\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def collate(inputs):\n",
    "    \"\"\"\n",
    "    It truncates the inputs to the maximum sequence length in the batch. \n",
    "    \"\"\"\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max()) # Get batch's max sequence length\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "    return inputs\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, cfg, df, tokenizer):\n",
    "        self.cfg = cfg\n",
    "        self.texts = df['text'].values\n",
    "        self.labels = df['label'].values\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        output = {}\n",
    "        output[\"inputs\"] = prepare_input(self.cfg, self.texts[item], self.tokenizer)\n",
    "        output[\"labels\"] = torch.tensor(self.labels[item], dtype=torch.float) # TODO: check dtypes\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a261bfe1-afc4-4339-9aa2-1c7cec458e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "    \n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.dropout = 0.2\n",
    "        # Load config by inferencing it from the model name.\n",
    "        if config_path is None: \n",
    "            self.config = AutoConfig.from_pretrained(cfg.MODEL, output_hidden_states=True)\n",
    "            self.config.hidden_dropout = 0.\n",
    "            self.config.hidden_dropout_prob = 0.\n",
    "            self.config.attention_dropout = 0.\n",
    "            self.config.attention_probs_dropout_prob = 0.\n",
    "        # Load config from a file.\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        \n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(cfg.MODEL, config=self.config)\n",
    "        else:\n",
    "            self.model = AutoModel(self.config)\n",
    "        \n",
    "        if self.cfg.GRADIENT_CHECKPOINTING:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "          \n",
    "        # Add MeanPooling and Linear head at the end to transform the Model into a RegressionModel\n",
    "        self.pool = MeanPooling()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.config.hidden_size, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "        self._init_weights(self.head)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"\n",
    "        This method initializes weights for different types of layers. The type of layers \n",
    "        supported are nn.Linear, nn.Embedding and nn.LayerNorm.\n",
    "        \"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "    def feature(self, inputs):\n",
    "        \"\"\"\n",
    "        This method makes a forward pass through the model, get the last hidden state (embedding)\n",
    "        and pass it through the MeanPooling layer.\n",
    "        \"\"\"\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        feature = self.pool(last_hidden_states, inputs['attention_mask'])\n",
    "        return feature\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        This method makes a forward pass through the model, the MeanPooling layer and finally\n",
    "        then through the Linear layer to get a regression value.\n",
    "        \"\"\"\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.head(feature)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a2ea638-01f3-4ee0-8213-1559b6a0decb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def train_epoch(train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    \"\"\"One epoch training pass.\"\"\"\n",
    "    model.train() # set model in train mode\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=config.APEX) # Automatic Mixed Precision tries to match each op to its appropriate datatype.\n",
    "    losses = AverageMeter() # initiate AverageMeter to track the loss.\n",
    "    start = end = time.time() # track the execution time.\n",
    "    global_step = 0\n",
    "    \n",
    "    # ========== ITERATE OVER TRAIN BATCHES ============\n",
    "    with tqdm(train_loader, unit=\"train_batch\", desc='Train') as tqdm_train_loader:\n",
    "        for step, batch in enumerate(tqdm_train_loader):\n",
    "            inputs = batch.pop(\"inputs\")\n",
    "            labels = batch.pop(\"labels\")\n",
    "            inputs = collate(inputs) # collate inputs\n",
    "            for k, v in inputs.items(): # send each tensor value to `device`\n",
    "                inputs[k] = v.to(device)\n",
    "            labels = labels.to(device) # send labels to `device`\n",
    "            batch_size = labels.size(0)\n",
    "            with torch.cuda.amp.autocast(enabled=config.APEX):\n",
    "                y_preds = model(inputs) # forward propagation pass\n",
    "                loss = criterion(y_preds, labels.unsqueeze(1)) # get loss\n",
    "            if config.GRADIENT_ACCUMULATION_STEPS > 1:\n",
    "                loss = loss / config.GRADIENT_ACCUMULATION_STEPS\n",
    "            losses.update(loss.item(), batch_size) # update loss function tracking\n",
    "            scaler.scale(loss).backward() # backward propagation pass\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.MAX_GRAD_NORM)\n",
    "\n",
    "            if (step + 1) % config.GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                scaler.step(optimizer) # update optimizer parameters\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad() # zero out the gradients\n",
    "                global_step += 1\n",
    "                if config.BATCH_SCHEDULER:\n",
    "                    scheduler.step() # update learning rate\n",
    "            end = time.time() # get finish time\n",
    "\n",
    "            # ========== LOG INFO ==========\n",
    "            if step % config.PRINT_FREQ == 0 or step == (len(train_loader)-1):\n",
    "                print('Epoch: [{0}][{1}/{2}] '\n",
    "                      'Elapsed {remain:s} '\n",
    "                      'Loss: {loss.avg:.4f} '\n",
    "                      'Grad: {grad_norm:.4f}  '\n",
    "                      'LR: {lr:.8f}  '\n",
    "                      .format(epoch+1, step, len(train_loader), \n",
    "                              remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                              loss=losses,\n",
    "                              grad_norm=grad_norm,\n",
    "                              lr=scheduler.get_lr()[0]))\n",
    "            if config.WANDB:\n",
    "                wandb.log({f\"[fold_{fold}] train loss\": losses.val,\n",
    "                           f\"[fold_{fold}] lr\": scheduler.get_lr()[0]})\n",
    "\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_epoch(valid_loader, model, criterion, device):\n",
    "    model.eval() # set model in evaluation mode\n",
    "    losses = AverageMeter() # initiate AverageMeter for tracking the loss.\n",
    "    prediction_dict = {}\n",
    "    preds = []\n",
    "    start = end = time.time() # track the execution time.\n",
    "    with tqdm(valid_loader, unit=\"valid_batch\", desc='Validation') as tqdm_valid_loader:\n",
    "        for step, batch in enumerate(tqdm_valid_loader):\n",
    "            inputs = batch.pop(\"inputs\")\n",
    "            labels = batch.pop(\"labels\")\n",
    "            inputs = collate(inputs) # collate inputs\n",
    "            for k, v in inputs.items():\n",
    "                inputs[k] = v.to(device) # send inputs to device\n",
    "            labels = labels.to(device)\n",
    "            batch_size = labels.size(0)\n",
    "            ids = torch.arange(start=step * batch_size, end=(step + 1) * batch_size).to(device)\n",
    "            with torch.no_grad():\n",
    "                y_preds = model(inputs) # forward propagation pass\n",
    "                loss = criterion(y_preds, labels.unsqueeze(1)) # get loss\n",
    "            if config.GRADIENT_ACCUMULATION_STEPS > 1:\n",
    "                loss = loss / config.GRADIENT_ACCUMULATION_STEPS\n",
    "            losses.update(loss.item(), batch_size) # update loss function tracking\n",
    "            preds.append(y_preds.to('cpu').numpy()) # save predictions\n",
    "            end = time.time() # get finish time\n",
    "\n",
    "            # ========== LOG INFO ==========\n",
    "            if step % config.PRINT_FREQ == 0 or step == (len(valid_loader)-1):\n",
    "                print('EVAL: [{0}/{1}] '\n",
    "                      'Elapsed {remain:s} '\n",
    "                      'Loss: {loss.avg:.4f} '\n",
    "                      .format(step, len(valid_loader),\n",
    "                              loss=losses,\n",
    "                              remain=timeSince(start, float(step+1)/len(valid_loader))))\n",
    "            if config.WANDB:\n",
    "                wandb.log({f\"[fold_{fold}] val loss\": losses.val})\n",
    "                \n",
    "    prediction_dict[\"predictions\"] = np.concatenate(preds) # np.array() of shape (fold_size, target_cols)\n",
    "    prediction_dict[\"ids\"] = ids.cpu().numpy()\n",
    "    return losses.avg, prediction_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3278a36-3d4c-4752-bd14-990d9e8a2187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(folds, fold):\n",
    "    \n",
    "    LOGGER.info(f\"========== Fold: {fold} training ==========\")\n",
    "\n",
    "    # ======== SPLIT ==========\n",
    "    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n",
    "    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n",
    "\n",
    "    valid_labels = valid_folds['label'].values\n",
    "\n",
    "    # ======== DATASETS ==========\n",
    "    train_dataset = CustomDataset(config, train_folds, tokenizer)\n",
    "    valid_dataset = CustomDataset(config, valid_folds, tokenizer)\n",
    "    \n",
    "    # ======== DATALOADERS ==========\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=config.BATCH_SIZE_TRAIN, # TODO: split into train and valid\n",
    "                              shuffle=True,\n",
    "                              pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=config.BATCH_SIZE_VALID,\n",
    "                              shuffle=False,\n",
    "                              pin_memory=True, drop_last=False)\n",
    "    \n",
    "    # ======== MODEL ==========\n",
    "    model = CustomModel(config, config_path=None, pretrained=True)\n",
    "    torch.save(model.config, paths.OUTPUT_DIR + '/config.pth')\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer_parameters = get_optimizer_params(model,\n",
    "                                                encoder_lr=config.ENCODER_LR, \n",
    "                                                decoder_lr=config.DECODER_LR,\n",
    "                                                weight_decay=config.WEIGHT_DECAY)\n",
    "    optimizer = AdamW(optimizer_parameters,\n",
    "                      lr=config.ENCODER_LR,\n",
    "                      eps=config.EPS,\n",
    "                      betas=config.BETAS)\n",
    "    \n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=1e-5,\n",
    "        epochs=config.EPOCHS,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        pct_start=0.1,\n",
    "        anneal_strategy=\"cos\",\n",
    "        final_div_factor=100,\n",
    "    )\n",
    "\n",
    "    # ======= LOSS ==========\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    best_score = -np.inf\n",
    "    # ====== ITERATE EPOCHS ========\n",
    "    for epoch in range(config.EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # ======= TRAIN ==========\n",
    "        avg_loss = train_epoch(train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
    "\n",
    "        # ======= EVALUATION ==========\n",
    "        avg_val_loss, prediction_dict = valid_epoch(valid_loader, model, criterion, device)\n",
    "        predictions = prediction_dict[\"predictions\"]\n",
    "        # ======= SCORING ==========\n",
    "        score = get_score(valid_labels, sigmoid(predictions))\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n",
    "        \n",
    "        if config.WANDB:\n",
    "            wandb.log({f\"[fold_{fold}] epoch\": epoch+1, \n",
    "                       f\"[fold_{fold}] avg_train_loss\": avg_loss, \n",
    "                       f\"[fold_{fold}] avg_val_loss\": avg_val_loss,\n",
    "                       f\"[fold_{fold}] score\": score})\n",
    "            \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save(model.state_dict(),\n",
    "                        paths.OUTPUT_DIR + f\"/{config.MODEL.replace('/', '_')}_fold_{fold}_best.pth\")\n",
    "            best_model_predictions = predictions\n",
    "\n",
    "    valid_folds[\"preds\"] = best_model_predictions\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return valid_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fe27a1a-6c7a-4ad2-8c65-965bbb7119d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== Fold: 0 training ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a98f8b8754445a0ae2bc0c5a3e09ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/672 [00:00<?, ?train_batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/672] Elapsed 0m 1s (remain 12m 2s) Loss: 0.7137 Grad: 215192.5000  LR: 0.00000040  \n",
      "Epoch: [1][20/672] Elapsed 0m 21s (remain 11m 19s) Loss: 0.7278 Grad: 236519.7500  LR: 0.00000049  \n",
      "Epoch: [1][40/672] Elapsed 0m 42s (remain 10m 58s) Loss: 0.7211 Grad: 318569.7500  LR: 0.00000075  \n",
      "Epoch: [1][60/672] Elapsed 1m 3s (remain 10m 37s) Loss: 0.7182 Grad: 219659.6250  LR: 0.00000116  \n",
      "Epoch: [1][80/672] Elapsed 1m 24s (remain 10m 17s) Loss: 0.7153 Grad: 249998.7656  LR: 0.00000172  \n",
      "Epoch: [1][100/672] Elapsed 1m 45s (remain 9m 56s) Loss: 0.7155 Grad: 226267.7500  LR: 0.00000240  \n",
      "Epoch: [1][120/672] Elapsed 2m 6s (remain 9m 35s) Loss: 0.7155 Grad: 282572.0000  LR: 0.00000317  \n",
      "Epoch: [1][140/672] Elapsed 2m 27s (remain 9m 15s) Loss: 0.7167 Grad: 218325.3125  LR: 0.00000402  \n",
      "Epoch: [1][160/672] Elapsed 2m 48s (remain 8m 55s) Loss: 0.7153 Grad: 228138.8438  LR: 0.00000491  \n",
      "Epoch: [1][180/672] Elapsed 3m 10s (remain 8m 35s) Loss: 0.7164 Grad: 226217.2188  LR: 0.00000581  \n",
      "Epoch: [1][200/672] Elapsed 3m 31s (remain 8m 16s) Loss: 0.7164 Grad: 307819.8125  LR: 0.00000668  \n",
      "Epoch: [1][220/672] Elapsed 3m 53s (remain 7m 56s) Loss: 0.7149 Grad: 242703.9688  LR: 0.00000751  \n",
      "Epoch: [1][240/672] Elapsed 4m 14s (remain 7m 35s) Loss: 0.7148 Grad: 176937.0781  LR: 0.00000825  \n",
      "Epoch: [1][260/672] Elapsed 4m 36s (remain 7m 15s) Loss: 0.7136 Grad: 183483.8125  LR: 0.00000889  \n",
      "Epoch: [1][280/672] Elapsed 4m 57s (remain 6m 54s) Loss: 0.7132 Grad: 217949.4844  LR: 0.00000940  \n",
      "Epoch: [1][300/672] Elapsed 5m 19s (remain 6m 33s) Loss: 0.7125 Grad: 250537.8906  LR: 0.00000976  \n",
      "Epoch: [1][320/672] Elapsed 5m 40s (remain 6m 12s) Loss: 0.7119 Grad: 186254.4688  LR: 0.00000996  \n",
      "Epoch: [1][340/672] Elapsed 6m 2s (remain 5m 51s) Loss: 0.7118 Grad: 236738.2344  LR: 0.00001000  \n",
      "Epoch: [1][360/672] Elapsed 6m 23s (remain 5m 30s) Loss: 0.7122 Grad: 215751.0312  LR: 0.00001000  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mFOLDS):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fold \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 12\u001b[0m         _oof_df \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m         oof_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([oof_df, _oof_df])\n\u001b[1;32m     14\u001b[0m         LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m========== Fold: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m result ==========\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 59\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(folds, fold)\u001b[0m\n\u001b[1;32m     56\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# ======= TRAIN ==========\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# ======= EVALUATION ==========\u001b[39;00m\n\u001b[1;32m     62\u001b[0m avg_val_loss, prediction_dict \u001b[38;5;241m=\u001b[39m valid_epoch(valid_loader, model, criterion, device)\n",
      "Cell \u001b[0;32mIn[20], line 61\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(train_loader, model, criterion, optimizer, epoch, scheduler, device)\u001b[0m\n\u001b[1;32m     58\u001b[0m grad_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), config\u001b[38;5;241m.\u001b[39mMAX_GRAD_NORM)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m config\u001b[38;5;241m.\u001b[39mGRADIENT_ACCUMULATION_STEPS \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 61\u001b[0m     \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# update optimizer parameters\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     63\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# zero out the gradients\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:452\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    450\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 452\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    454\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:349\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    343\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    347\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    348\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    350\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:349\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    343\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    347\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    348\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    350\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    def get_result(oof_df):\n",
    "        labels = oof_df[\"label\"].values\n",
    "        preds = oof_df[\"preds\"].values\n",
    "        score = get_score(labels, preds)\n",
    "        LOGGER.info(f'Score: {score:<.4f}')\n",
    "    \n",
    "    if config.TRAIN:\n",
    "        oof_df = pd.DataFrame()\n",
    "        for fold in range(config.FOLDS):\n",
    "            if fold == 0:\n",
    "                _oof_df = train_loop(train_df, fold)\n",
    "                oof_df = pd.concat([oof_df, _oof_df])\n",
    "                LOGGER.info(f\"========== Fold: {fold} result ==========\")\n",
    "                get_result(_oof_df)\n",
    "        oof_df = oof_df.reset_index(drop=True)\n",
    "        LOGGER.info(f\"========== CV ==========\")\n",
    "        get_result(oof_df)\n",
    "        oof_df.to_csv(paths.OUTPUT_DIR + '/oof_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d26ff2f-d657-4d04-a42e-37f4d7f66c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4844e818-3226-43d0-8445-c31f0d6c6da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_df[\"preds\"] = oof_df[\"preds\"].apply(lambda x: sigmoid(x))\n",
    "oof_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d169078f-6fb6-4ffe-9da8-139e6f6bfa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def binarize(x, threshold):\n",
    "    if x > threshold:\n",
    "        x = 1\n",
    "    else:\n",
    "        x = 0\n",
    "    return x\n",
    "\n",
    "# Assuming df is your pandas DataFrame\n",
    "oof_df[\"binary\"] = oof_df[\"preds\"].apply(lambda x: binarize(x, 0.5))\n",
    "true_labels = oof_df[\"label\"].values\n",
    "predicted_labels = oof_df[\"binary\"].values\n",
    "\n",
    "# Get the unique classes from both true and predicted labels\n",
    "classes = np.unique(np.concatenate((true_labels, predicted_labels)))\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels, labels=classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fd86fb-7747-4ba5-a5b0-64dd0cb3b89d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
